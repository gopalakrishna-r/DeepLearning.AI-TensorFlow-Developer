{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\r\n",
    "from keras.layers.experimental import preprocessing\r\n",
    "\r\n",
    "import numpy\r\n",
    "import os\r\n",
    "import time\r\n",
    "\r\n",
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "download the shakespeare dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt' )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "read the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n",
    "\r\n",
    "print(f'Length of text: {len(text)} characters')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(text[:250])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "vocab = sorted(set(text))\r\n",
    "print(f'{len(vocab)} unique characters')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Process the text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectorize the text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "example_texts = ['abcdefg', 'xyz']\r\n",
    "\r\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding = 'UTF-8')\r\n",
    "chars"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ids_from_chars  = preprocessing.StringLookup(vocabulary= list(vocab), mask_token=None)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'keras.layers.experimental.preprocessing' has no attribute 'StringLookup'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1c94265b904b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mids_from_chars\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStringLookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.layers.experimental.preprocessing' has no attribute 'StringLookup'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ids = ids_from_chars(chars)\r\n",
    "ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary= ids_from_chars.get_vocabulary(), invert = True, mask_token = None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "chars = chars_from_ids(ids)\r\n",
    "chars"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.strings.reduce_join(chars, axis = -1).numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def text_from_ids(ids):\r\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\r\n",
    "all_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seq_length = 100\r\n",
    "examples_per_epoch = len(text) // (seq_length + 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder = True )\r\n",
    "\r\n",
    "for seq in sequences.take(5):\r\n",
    "    print(text_from_ids(seq).numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_input_target(sequence):\r\n",
    "    input_text = sequence[:-1]\r\n",
    "    target_text = sequence[1:]\r\n",
    "    return input_text, target_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = sequences.map(split_input_target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for input_example, target_example in dataset.take(1):\r\n",
    "    print(\"Input : \", text_from_ids(input_example).numpy())\r\n",
    "    print(\"Target : \", text_from_ids(target_example).numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create treaining batches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Batch size\r\n",
    "BATCH_SIZE = 64\r\n",
    "\r\n",
    "BUFFER_SIZE = 10000\r\n",
    "\r\n",
    "dataset = (dataset\r\n",
    "            .shuffle(BUFFER_SIZE)\r\n",
    "            .batch(BATCH_SIZE, drop_remainder = True)\r\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE))\r\n",
    "\r\n",
    "dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab_size = len(vocab)\r\n",
    "\r\n",
    "embedding_dim = 256\r\n",
    "\r\n",
    "rnn_units = 1024"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MyModel(tf.keras.Model):\r\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\r\n",
    "        super().__init__(self)\r\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units, \r\n",
    "                    return_sequences = True, \r\n",
    "                    return_state = True)\r\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
    "    \r\n",
    "    def call(self, inputs, states = None, return_state = False, training = False):\r\n",
    "        x = inputs\r\n",
    "        x = self.embedding(x, training = training)\r\n",
    "        if states is None:\r\n",
    "            states = self.gru.get_initial_state(x)\r\n",
    "        x, states = self.gru(x, initial_state = states, training = training)\r\n",
    "        x = self.dense(x, training = training)\r\n",
    "\r\n",
    "        if return_state:\r\n",
    "            return x, states\r\n",
    "        else:\r\n",
    "            return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = MyModel(vocab_size = len(ids_from_chars.get_vocabulary()), embedding_dim = embedding_dim, rnn_units = rnn_units)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
    "    example_batch_predictions = model(input_example_batch)\r\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples = 1)\r\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()\r\n",
    "sampled_indices"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Input:\\n \", text_from_ids(input_example_batch[0]).numpy())\r\n",
    "print()\r\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\r\n",
    "mean_loss = example_batch_loss.numpy().mean()\r\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n",
    "print(\"Mean loss:        \", mean_loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.exp(mean_loss).numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(optimizer='adam', loss = loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint_dir = './training_checkpoints'\r\n",
    "\r\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\r\n",
    "\r\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute the training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "EPOCHS = 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(dataset, epochs = EPOCHS, callbacks =  [checkpoint_callback])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class OneStep(tf.keras.Model):\r\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\r\n",
    "    super().__init__()\r\n",
    "    self.temperature = temperature\r\n",
    "    self.model = model\r\n",
    "    self.chars_from_ids = chars_from_ids\r\n",
    "    self.ids_from_chars = ids_from_chars\r\n",
    "\r\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\r\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\r\n",
    "    sparse_mask = tf.SparseTensor(\r\n",
    "        # Put a -inf at each bad index.\r\n",
    "        values=[-float('inf')]*len(skip_ids),\r\n",
    "        indices=skip_ids,\r\n",
    "        # Match the shape to the vocabulary\r\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\r\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\r\n",
    "\r\n",
    "  @tf.function\r\n",
    "  def generate_one_step(self, inputs, states=None):\r\n",
    "    # Convert strings to token IDs.\r\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\r\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\r\n",
    "\r\n",
    "    # Run the model.\r\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\r\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\r\n",
    "                                          return_state=True)\r\n",
    "    # Only use the last prediction.\r\n",
    "    predicted_logits = predicted_logits[:, -1, :]\r\n",
    "    predicted_logits = predicted_logits/self.temperature\r\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\r\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\r\n",
    "\r\n",
    "    # Sample the output logits to generate token IDs.\r\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\r\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\r\n",
    "\r\n",
    "    # Convert from token ids to characters\r\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\r\n",
    "\r\n",
    "    # Return the characters and model state.\r\n",
    "    return predicted_chars, states"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start = time.time()\r\n",
    "states = None\r\n",
    "next_char = tf.constant(['ROMEO:'])\r\n",
    "result = [next_char]\r\n",
    "\r\n",
    "for n in range(1000):\r\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
    "  result.append(next_char)\r\n",
    "\r\n",
    "result = tf.strings.join(result)\r\n",
    "end = time.time()\r\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\r\n",
    "print('\\nRun time:', end - start)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start = time.time()\r\n",
    "states = None\r\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\r\n",
    "result = [next_char]\r\n",
    "\r\n",
    "for n in range(1000):\r\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
    "  result.append(next_char)\r\n",
    "\r\n",
    "result = tf.strings.join(result)\r\n",
    "end = time.time()\r\n",
    "print(result, '\\n\\n' + '_'*80)\r\n",
    "print('\\nRun time:', end - start)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\r\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "states = None\r\n",
    "next_char = tf.constant(['ROMEO:'])\r\n",
    "result = [next_char]\r\n",
    "\r\n",
    "for n in range(100):\r\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\r\n",
    "  result.append(next_char)\r\n",
    "\r\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "360eb45faca1e4dfefc4f13aa9499776008d91528b4d443d812d58097d713eb4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}